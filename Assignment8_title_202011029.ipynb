{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment8_title_202011029.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSSTmB6JXftN"
      },
      "source": [
        "Ensemle based classification performed on titles of documents. 5-fold cross-validation is used.\n",
        "\n",
        "*   Accuracy of Stacking Classifier on titles: 93.65\n",
        "*   Accuracy of Voting Classifier on titles: 93.5\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   Accuracy of Multinomial Naive Bayes on titles: 87.37\n",
        "*   Accuracy of Random Forest classifier on titles: 93.65\n",
        "*   Accuracy of Logistic Regression on titles: 92.31\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVjpCQo1UjkN"
      },
      "source": [
        "# Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6la5UMWkruxg",
        "outputId": "65238629-4f03-461f-da60-22d9cf07248d"
      },
      "source": [
        "!gdown --id 1q0ZimHCtMlhftljfVuy0i-wcgPnDUYjl "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1q0ZimHCtMlhftljfVuy0i-wcgPnDUYjl\n",
            "To: /content/fake-news.zip\n",
            "48.7MB [00:01, 47.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwS5nsFeUkqZ"
      },
      "source": [
        "# Extracting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWiBo64nwYUe",
        "outputId": "25a45a94-e086-4583-8102-144efe29c720"
      },
      "source": [
        "!unzip /content/fake-news.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/fake-news.zip\n",
            "  inflating: submit.csv              \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvncnGzU_j-"
      },
      "source": [
        "# Importing and Downloading necessary packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA3_NNJ6xxIu",
        "outputId": "d75f9689-5269-40be-c2a0-749d547ec4a7"
      },
      "source": [
        "import numpy as np    \n",
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import numpy as np\n",
        "import statistics\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7EIAXFlVCaH"
      },
      "source": [
        "# Data from csv files\n",
        "In this section, data is extracted from train.csv and test.csv file using pandas and stored in a list where each row contains all the fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9_sxph1whC1"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "train_data = train.iloc[:,:].values\n",
        "test = pd.read_csv('test.csv')\n",
        "test_data = test.iloc[:,:].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx6AEWG_VQzJ"
      },
      "source": [
        "# Data Preprocessing\n",
        "In this section, data preprocessing is done on titles of training data. Also appending training labels in this section in list named y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgIUrrM_sGby",
        "outputId": "ccf50a88-1b51-49f6-aacf-161f6dae5f24"
      },
      "source": [
        "files = []\n",
        "corpus = []\n",
        "labels = train_data[:,-1]\n",
        "y = []\n",
        "\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "postag = nltk.corpus.wordnet\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for row in range(len(train_data)) :\n",
        "  raw_text = train_data[row][1]\n",
        "  try:\n",
        "    for sym in raw_text : \n",
        "      # Removing punctuation\n",
        "      if sym in punc : \n",
        "        raw_text = raw_text.replace(sym, \"\")\n",
        "    # Removing non-alphabetic text and Converting text to lower-case\n",
        "    words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "    # Removing stopwords\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    # Performing lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "    temp_files = []\n",
        "    temp_files.append(train_data[row][0])\n",
        "    temp_files.append(lemmatized)\n",
        "    # Storing file data to list\n",
        "    files.append(temp_files)\n",
        "    doc_text = \"\"\n",
        "    for temp_str in lemmatized:\n",
        "      doc_text = doc_text + temp_str + \" \"\n",
        "    corpus.append(doc_text)\n",
        "    # appending labels of rows that have data\n",
        "    y.append(labels[row])\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "print(\"Total Documents: \", len(files))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  20242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y3e63PbVqZp"
      },
      "source": [
        "# Creating document vectors of corpus using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpZfy-veuisq"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgxkzAAxWAOz"
      },
      "source": [
        "# Text preprocessing on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7yzBqsfGTt9",
        "outputId": "5d4da5a2-2bf1-4353-b29f-187276f76e6a"
      },
      "source": [
        "files2 = []\n",
        "corpus = []\n",
        "\n",
        "for row in test_data :\n",
        "  raw_text = row[1]\n",
        "  try:\n",
        "    for sym in raw_text : \n",
        "      # Removing punctuation\n",
        "      if sym in punc : \n",
        "        raw_text = raw_text.replace(sym, \"\")\n",
        "    # Removing non-alphabetic text and Converting text to lower-case\n",
        "    words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "    # Removing stopwords\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    # Performing lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "    temp_files = []\n",
        "    temp_files.append(row[0])\n",
        "    temp_files.append(lemmatized)\n",
        "    # Storing file data to list\n",
        "    files2.append(temp_files)\n",
        "\n",
        "    doc_text = \"\"\n",
        "    for temp_str in lemmatized:\n",
        "      doc_text = doc_text + temp_str + \" \"\n",
        "    corpus.append(doc_text)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "print(\"Total Documents: \", len(files2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  5078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2-ISMNWWHFW"
      },
      "source": [
        "# Creating document vectors of test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUft8JwkIcwY"
      },
      "source": [
        "X_test = vectorizer.transform(corpus)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4xauB9ZWKk9"
      },
      "source": [
        "# Implementing individual classifiers\n",
        "In this section, Multinomial Naive Bayes, Random Forest Classifier and Logistic Regression on titles data. Accuracy on training data is also acquired in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfx5jwcsIhc4",
        "outputId": "3212a48c-2ad1-4a7e-d40c-e20077ae5b0a"
      },
      "source": [
        "scores = []\n",
        "\n",
        "clf1 = MultinomialNB()\n",
        "clf2 = RandomForestClassifier(random_state=1)\n",
        "clf3 = LogisticRegression()\n",
        "\n",
        "clf1.fit(X, y)\n",
        "scores.append(model_selection.cross_val_score(clf1, X, y, cv=5, scoring='accuracy'))\n",
        "print(\"Accuracy of Multinomial Naive Bayes on titles:\", round(statistics.mean(scores[0])*100,2))\n",
        "clf2.fit(X, y)\n",
        "scores.append(model_selection.cross_val_score(clf2, X, y, cv=5, scoring='accuracy'))\n",
        "print(\"Accuracy of Random Forest classifier on titles:\", round(statistics.mean(scores[1])*100,2))\n",
        "clf3.fit(X, y)\n",
        "scores.append(model_selection.cross_val_score(clf3, X, y, cv=5, scoring='accuracy'))\n",
        "print(\"Accuracy of Logistic Regression on titles:\", round(statistics.mean(scores[2])*100,2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Multinomial Naive Bayes on titles: 87.37\n",
            "Accuracy of Random Forest classifier on titles: 93.65\n",
            "Accuracy of Logistic Regression on titles: 92.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m7MVJxiW92T"
      },
      "source": [
        "# Implementing Stacking Classifier and Voting Classifier\n",
        "In stacking classifier, logistic regression is put as meta classifier and in voting classifier, voting is kept as hard. Accuracy of both the sections is also calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnVtCRvYlOGL",
        "outputId": "93efc348-3e3c-435e-de31-67bfd9be02fe"
      },
      "source": [
        "clf4 = LogisticRegression()\n",
        "\n",
        "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=clf4)\n",
        "sclf.fit(X, y)\n",
        "s_score = (model_selection.cross_val_score(sclf, X, y, cv=5, scoring='accuracy'))\n",
        "\n",
        "vclf = VotingClassifier(estimators=[('mnb', clf1), ('rf', clf2), ('lr', clf3)], voting='hard')\n",
        "vclf.fit(X, y)\n",
        "v_score = (model_selection.cross_val_score(vclf, X, y, cv=5, scoring='accuracy'))\n",
        "print(\"Accuracy of Stacking Classifier on titles:\", round(statistics.mean(s_score)*100,2))\n",
        "print(\"Accuracy of Voting Classifier on titles:\", round(statistics.mean(v_score)*100,2))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Stacking Classifier on titles: 93.65\n",
            "Accuracy of Voting Classifier on titles: 93.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_ovRZy4XVc6"
      },
      "source": [
        "# Two output files are made for both the classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKzBOrzoT1AQ"
      },
      "source": [
        "final_csv_s = []\n",
        "final_csv_v = []\n",
        "\n",
        "sy_pred = sclf.predict(X_test)\n",
        "vy_pred = vclf.predict(X_test)\n",
        "\n",
        "for i in range(len(sy_pred)):\n",
        "    final_csv_s.append([files2[i][0], sy_pred[i]])\n",
        "    final_csv_v.append([files2[i][0], vy_pred[i]])\n",
        "\n",
        "with open('title_stacking_output.csv','w') as f1:\n",
        "    writer = csv.writer(f1)\n",
        "    writer.writerows(final_csv_s)\n",
        "\n",
        "with open('title_voting_output.csv','w') as f2:\n",
        "    writer = csv.writer(f2)\n",
        "    writer.writerows(final_csv_v)"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}