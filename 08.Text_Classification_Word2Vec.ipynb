{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7_word2vec_202011029.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjY7fd3X2ks0"
      },
      "source": [
        "# Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ3I4WcD3ASo",
        "outputId": "23690416-a4e7-49e9-e065-397594ffd6dc"
      },
      "source": [
        "!gdown --id 18l6IwSqavnqtLQpVnrRqOugZf9XkhEAN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18l6IwSqavnqtLQpVnrRqOugZf9XkhEAN\n",
            "To: /content/jigsaw-toxic-comment-classification-challenge.zip\n",
            "55.2MB [00:00, 82.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWTPOv6Y2oML"
      },
      "source": [
        "# Extracting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoRJNfA3n6S",
        "outputId": "05f03d06-06f1-4dca-a582-fde7358d51ad"
      },
      "source": [
        "!unzip jigsaw-toxic-comment-classification-challenge.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: test_labels.csv.zip     \n",
            "  inflating: train.csv.zip           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zvGmtfd4Zxs",
        "outputId": "2cb13710-58ea-483c-92b6-4628e231967a"
      },
      "source": [
        "!unzip sample_submission.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMFVSiIU4egQ",
        "outputId": "26a05ee9-bed7-4651-c766-50806270c18c"
      },
      "source": [
        "!unzip test.csv.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw28JShG4jmi",
        "outputId": "5759c2a2-5a8a-47a1-96ba-e15e56067589"
      },
      "source": [
        "!unzip test_labels.csv.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OxKx80Q4n9Q",
        "outputId": "16bd671c-00a4-4579-af54-65b5202ff2c2"
      },
      "source": [
        "!unzip train.csv.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzlBR2A62sSo"
      },
      "source": [
        "# Importing and Downloading necessary packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3wqVHYN4r0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b418fb1f-13a2-4654-fbef-831344e42ac1"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "import lxml.html\n",
        "import re\n",
        "import numpy as np    \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import bisect\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy\n",
        "from gensim.models import Word2Vec\n",
        "import csv\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P22idbiKQxNE"
      },
      "source": [
        "# Data from csv files\n",
        "In this section, data is extracted from train.csv and test.csv file using csv.reader and stores in a list where each row contains all the fields of each csv row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_qcPG941fL"
      },
      "source": [
        "fields1 = []\n",
        "train_data = []\n",
        "  \n",
        "with open(\"train.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields1 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    train_data.append(row)\n",
        "\n",
        "fields2 = []\n",
        "test_data = []\n",
        "  \n",
        "with open(\"test.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields2 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    test_data.append(row)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pqd9mXiRE37"
      },
      "source": [
        "# Data Preprocessing\n",
        "In this section, data preprocessing is done on training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CZc9EH3WPES",
        "outputId": "d5f75aa8-e241-4382-e7b2-7d392ab990bd"
      },
      "source": [
        "files = []\n",
        "corpus = []\n",
        "\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "postag = nltk.corpus.wordnet\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for row in train_data :\n",
        "  raw_text = row[1]\n",
        "  for sym in raw_text : \n",
        "    # Removing punctuation\n",
        "    if sym in punc : \n",
        "      raw_text = raw_text.replace(sym, \"\")\n",
        "  # Removing non-alphabetic text and Converting text to lower-case\n",
        "  words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "  # Removing stopwords\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  # Performing lemmatization\n",
        "  lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "  temp_files = []\n",
        "  temp_files.append(row[0])\n",
        "  temp_files.append(lemmatized)\n",
        "  # Storing file data to list\n",
        "  files.append(temp_files)\n",
        "  # Appending data to 'files' list\n",
        "  corpus.append(lemmatized)\n",
        "  \n",
        "print(\"Total Documents: \", len(files))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  159571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qRRApC9RPMm"
      },
      "source": [
        "# Word2Vec model training using CBOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncQTLVyYYVnw",
        "outputId": "365aa1c0-51e1-4ab0-ebdc-98e2de309a7b"
      },
      "source": [
        "vect_size = 100\n",
        "\n",
        "print(\"Training with CBOW started...\")\n",
        "# Training Word2Vec model using CBOW \n",
        "cbow_model = Word2Vec(corpus, size = vect_size, min_count = 2,window = 10, sg = 0, hs = 1, iter = 5, workers = 10)\n",
        "\n",
        "print(\"Training with CBOW done.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with CBOW started...\n",
            "Training with CBOW done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6bGRIURRWdA"
      },
      "source": [
        "# Document Vectors\n",
        "In this section, document vectors are created in which vector for each word is added in the row and then it is normalised with the length of that document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnnN86VdZMJ_",
        "outputId": "24493abb-60c7-487a-a2e4-05190def5ca9"
      },
      "source": [
        "print(\"Creating vectors for Documents using CBOW word2vec ...\")\n",
        "# Calculating weighted average and forming document vectors\n",
        "count = 0\n",
        "train_arr = []\n",
        "for i in corpus:\n",
        "    demo = np.zeros((vect_size,),dtype = float) # initializing a zero matrix\n",
        "    for j in i:\n",
        "        try:\n",
        "            demo = demo + cbow_model.wv.__getitem__(j) # adding values of each token in doc to vector\n",
        "        except:\n",
        "          continue\n",
        "    count = count + 1\n",
        "    try:\n",
        "      train_arr.append(demo/len(i))\n",
        "    except:\n",
        "      train_arr.append(demo)\n",
        "    if count%10000 == 0:\n",
        "      print(\"Progress:\", count, \"/\", len(corpus))\n",
        "\n",
        "print(\"Vectors created.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vectors for Documents using CBOW word2vec ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Progress: 10000 / 159571\n",
            "Progress: 20000 / 159571\n",
            "Progress: 30000 / 159571\n",
            "Progress: 40000 / 159571\n",
            "Progress: 50000 / 159571\n",
            "Progress: 60000 / 159571\n",
            "Progress: 70000 / 159571\n",
            "Progress: 80000 / 159571\n",
            "Progress: 90000 / 159571\n",
            "Progress: 100000 / 159571\n",
            "Progress: 110000 / 159571\n",
            "Progress: 120000 / 159571\n",
            "Progress: 130000 / 159571\n",
            "Progress: 140000 / 159571\n",
            "Progress: 150000 / 159571\n",
            "Vectors created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uk3mMHoR3E8"
      },
      "source": [
        "# Storing training labels\n",
        "In this section, all the six labels from training data are loaded in separate lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHsxaszaZuS"
      },
      "source": [
        "train_labels1 = []\n",
        "train_labels2 = []\n",
        "train_labels3 = []\n",
        "train_labels4 = []\n",
        "train_labels5 = []\n",
        "train_labels6 = []\n",
        "\n",
        "for row in train_data : \n",
        "  train_labels1.append(row[2])\n",
        "  train_labels2.append(row[3])\n",
        "  train_labels3.append(row[4])\n",
        "  train_labels4.append(row[5])\n",
        "  train_labels5.append(row[6])\n",
        "  train_labels6.append(row[7])\n",
        "\n",
        "\n",
        "y1 = np.array(train_labels1)\n",
        "y2 = np.array(train_labels2)\n",
        "y3 = np.array(train_labels3)\n",
        "y4 = np.array(train_labels4)\n",
        "y5 = np.array(train_labels5)\n",
        "y6 = np.array(train_labels6)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLbkK5bbSMR6"
      },
      "source": [
        "# Scaling of training data for proper execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Sf_AGCjSLU"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "X = np.array(train_arr)\n",
        "X = np.nan_to_num(X)\n",
        "scaler = StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmnQaZLUSYam"
      },
      "source": [
        "# Implementing SVM for classification using LinearSVC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4rPtimLpV2a",
        "outputId": "4787c31e-6ef4-47b7-b97e-5089b06365b7"
      },
      "source": [
        "clf1 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf1.fit(X_scaled, y1)\n",
        "\n",
        "clf2 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf2.fit(X_scaled, y2)\n",
        "\n",
        "clf3 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf3.fit(X_scaled, y3)\n",
        "\n",
        "clf4 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf4.fit(X_scaled, y4)\n",
        "\n",
        "clf5 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf5.fit(X_scaled, y5)\n",
        "\n",
        "clf6 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf6.fit(X_scaled, y6)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('linearsvc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=100,\n",
              "                           multi_class='ovr', penalty='l2', random_state=0,\n",
              "                           tol=1e-05, verbose=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QafmVpO7SgAR"
      },
      "source": [
        "# Accuracy of trained data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp42vNCFpV2b",
        "outputId": "65ee3da8-4859-4015-d947-42b176fa5ede"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Optional training accuracy\n",
        "\n",
        "predicted = clf1.predict(X_scaled)\n",
        "print(\"Training accuracy using sklearn LinearSVC:\",round(accuracy_score(y1, predicted)*100,2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy using sklearn LinearSVC: 94.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4JeHfw-SlIz"
      },
      "source": [
        "# Text preprocessing on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91NHvlSJpV2b",
        "outputId": "d059eb18-d702-40a8-c1c2-883b7859c68b"
      },
      "source": [
        "files2 = []\n",
        "corpus2 = []\n",
        "\n",
        "for row in test_data :\n",
        "  raw_text = row[1]\n",
        "  for sym in raw_text : \n",
        "    # Removing punctuation\n",
        "    if sym in punc : \n",
        "      raw_text = raw_text.replace(sym, \"\")\n",
        "  # Removing non-alphabetic text and Converting text to lower-case\n",
        "  words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "  # Removing stopwords\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  # Performing lemmatization\n",
        "  lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "  temp_files = []\n",
        "  temp_files.append(row[0])\n",
        "  temp_files.append(lemmatized)\n",
        "  # Storing file data to list\n",
        "  files2.append(temp_files)\n",
        "  # Appending data to 'files' list\n",
        "  corpus2.append(lemmatized)\n",
        "  \n",
        "print(\"Total Documents: \", len(files))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  159571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH-kfPGKSro6"
      },
      "source": [
        "# Creating vectors of test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkbZSmLRpV2c",
        "outputId": "3ff70629-4d52-4807-ff2d-00053c099b0b"
      },
      "source": [
        "print(\"Creating vectors for test data using CBOW word2vec ...\")\n",
        "# Calculating weighted average and forming document vectors\n",
        "count = 0\n",
        "test_arr = []\n",
        "for i in corpus2:\n",
        "    demo = np.zeros((vect_size,),dtype = float) # initializing a zero matrix\n",
        "    for j in i:\n",
        "        try:\n",
        "            demo = demo + cbow_model.wv.__getitem__(j) # adding values of each token in doc to vector\n",
        "        except:\n",
        "          continue\n",
        "    count = count + 1\n",
        "    try:\n",
        "      test_arr.append(demo/len(i))\n",
        "    except:\n",
        "      test_arr.append(demo)\n",
        "    if count%10000 == 0:\n",
        "      print(\"Progress:\", count, \"/\", len(corpus2))\n",
        "\n",
        "print(\"Vectors created.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating vectors for test data using CBOW word2vec ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Progress: 10000 / 153164\n",
            "Progress: 20000 / 153164\n",
            "Progress: 30000 / 153164\n",
            "Progress: 40000 / 153164\n",
            "Progress: 50000 / 153164\n",
            "Progress: 60000 / 153164\n",
            "Progress: 70000 / 153164\n",
            "Progress: 80000 / 153164\n",
            "Progress: 90000 / 153164\n",
            "Progress: 100000 / 153164\n",
            "Progress: 110000 / 153164\n",
            "Progress: 120000 / 153164\n",
            "Progress: 130000 / 153164\n",
            "Progress: 140000 / 153164\n",
            "Progress: 150000 / 153164\n",
            "Vectors created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_gn09g4Sv01"
      },
      "source": [
        "# Placing the predicted labels into output file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVvP9qCpV2d"
      },
      "source": [
        "X = np.array(test_arr)\n",
        "X = np.nan_to_num(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "predicted1 = clf1.predict(X_scaled)\n",
        "predicted2 = clf2.predict(X_scaled)\n",
        "predicted3 = clf3.predict(X_scaled)\n",
        "predicted4 = clf4.predict(X_scaled)\n",
        "predicted5 = clf5.predict(X_scaled)\n",
        "predicted6 = clf6.predict(X_scaled)\n",
        "\n",
        "\n",
        "final_csv = []\n",
        "header = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "for i in range(len(predicted1)):\n",
        "    final_csv.append([files2[i][0], predicted1[i], predicted2[i], predicted3[i], predicted4[i], predicted5[i], predicted6[i]])\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('output_w2v.csv','w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(final_csv)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Cz-_5AS9fs"
      },
      "source": [
        "# Loading the target labels from test_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqJAO72_pV2e"
      },
      "source": [
        "test_labels = []\n",
        "\n",
        "with open(\"test_labels.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields2 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    test_labels.append(row)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCwkfDqwTFq1"
      },
      "source": [
        "# Removing the rows with -1 value from target and predicted test labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmpGn8sipV2f"
      },
      "source": [
        "y_pred1 = []\n",
        "y_pred2 = []\n",
        "y_pred3 = []\n",
        "y_pred4 = []\n",
        "y_pred5 = []\n",
        "y_pred6 = []\n",
        "\n",
        "y_actual1 = []\n",
        "y_actual2 = []\n",
        "y_actual3 = []\n",
        "y_actual4 = []\n",
        "y_actual5 = []\n",
        "y_actual6 = []\n",
        "\n",
        "for i in range(len(predicted1)):\n",
        "    if(test_labels[i][1] != -1):\n",
        "        y_pred1.append(predicted1[i])\n",
        "        y_pred2.append(predicted2[i])\n",
        "        y_pred3.append(predicted3[i])\n",
        "        y_pred4.append(predicted4[i])\n",
        "        y_pred5.append(predicted5[i])\n",
        "        y_pred6.append(predicted6[i])\n",
        "\n",
        "        y_actual1.append(test_labels[i][1])\n",
        "        y_actual2.append(test_labels[i][2])\n",
        "        y_actual3.append(test_labels[i][3])\n",
        "        y_actual4.append(test_labels[i][4])\n",
        "        y_actual5.append(test_labels[i][5])\n",
        "        y_actual6.append(test_labels[i][6])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3fjzG7TLvI"
      },
      "source": [
        "# Finding out the Micro and Macro - Precision, Recall and F1Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9LyGHD3pV2g",
        "outputId": "dda863e7-e606-4fd4-e6ee-442377454f8d"
      },
      "source": [
        "m1 = precision_recall_fscore_support(y_actual1, y_pred1, average='micro')\n",
        "m2 = precision_recall_fscore_support(y_actual2, y_pred2, average='micro')\n",
        "m3 = precision_recall_fscore_support(y_actual3, y_pred3, average='micro')\n",
        "m4 = precision_recall_fscore_support(y_actual4, y_pred4, average='micro')\n",
        "m5 = precision_recall_fscore_support(y_actual5, y_pred5, average='micro')\n",
        "m6 = precision_recall_fscore_support(y_actual6, y_pred6, average='micro')\n",
        "\n",
        "print(\"Label 1 -> Micro-precision:\", round(m1[0], 2), \"\\tRecall:\", round(m1[1], 2), \"\\tFScore:\", round(m1[2], 2))\n",
        "print(\"Label 2 -> Micro-precision:\", round(m2[0], 2), \"\\tRecall:\", round(m2[1], 2), \"\\tFScore:\", round(m2[2], 2))\n",
        "print(\"Label 3 -> Micro-precision:\", round(m3[0], 2), \"\\tRecall:\", round(m3[1], 2), \"\\tFScore:\", round(m3[2], 2))\n",
        "print(\"Label 4 -> Micro-precision:\", round(m4[0], 2), \"\\tRecall:\", round(m4[1], 2), \"\\tFScore:\", round(m4[2], 2))\n",
        "print(\"Label 5 -> Micro-precision:\", round(m5[0], 2), \"\\tRecall:\", round(m5[1], 2), \"\\tFScore:\", round(m5[2], 2))\n",
        "print(\"Label 6 -> Micro-precision:\", round(m6[0], 2), \"\\tRecall:\", round(m6[1], 2), \"\\tFScore:\", round(m6[2], 2))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "ma1 = precision_recall_fscore_support(y_actual1, y_pred1, average='macro')\n",
        "ma2 = precision_recall_fscore_support(y_actual2, y_pred2, average='macro')\n",
        "ma3 = precision_recall_fscore_support(y_actual3, y_pred3, average='macro')\n",
        "ma4 = precision_recall_fscore_support(y_actual4, y_pred4, average='macro')\n",
        "ma5 = precision_recall_fscore_support(y_actual5, y_pred5, average='macro')\n",
        "ma6 = precision_recall_fscore_support(y_actual6, y_pred6, average='macro')\n",
        "\n",
        "print(\"Label 1 -> Macro-precision:\", round(ma1[0], 2), \"\\tRecall:\", round(ma1[1], 2), \"\\tFScore:\", round(ma1[2], 2))\n",
        "print(\"Label 2 -> Macro-precision:\", round(ma2[0], 2), \"\\tRecall:\", round(ma2[1], 2), \"\\tFScore:\", round(ma2[2], 2))\n",
        "print(\"Label 3 -> Macro-precision:\", round(ma3[0], 2), \"\\tRecall:\", round(ma3[1], 2), \"\\tFScore:\", round(ma3[2], 2))\n",
        "print(\"Label 4 -> Macro-precision:\", round(ma4[0], 2), \"\\tRecall:\", round(ma4[1], 2), \"\\tFScore:\", round(ma4[2], 2))\n",
        "print(\"Label 5 -> Macro-precision:\", round(ma5[0], 2), \"\\tRecall:\", round(ma5[1], 2), \"\\tFScore:\", round(ma5[2], 2))\n",
        "print(\"Label 6 -> Macro-precision:\", round(ma6[0], 2), \"\\tRecall:\", round(ma6[1], 2), \"\\tFScore:\", round(ma6[2], 2))\n",
        "\n",
        "aMiP = (m1[0] + m2[0] + m3[0] + m4[0] + m5[0] + m6[0])/6\n",
        "aMaP = (ma1[0] + ma2[0] + ma3[0] + ma4[0] + ma5[0] + ma6[0])/6\n",
        "aMiR = (m1[1] + m2[1] + m3[1] + m4[1] + m5[1] + m6[1])/6\n",
        "aMaR = (ma1[1] + ma2[1] + ma3[1] + ma4[1] + ma5[1] + ma6[1])/6\n",
        "aMiF = (m1[2] + m2[2] + m3[2] + m4[2] + m5[2] + m6[2])/6\n",
        "aMaF = (ma1[2] + ma2[2] + ma3[2] + ma4[2] + ma5[2] + ma6[2])/6\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Average Micro Precision:\",round(aMiP,2))\n",
        "print(\"Average Macro Precision:\",round(aMaP,2))\n",
        "\n",
        "print(\"Average Micro Recall:\",round(aMiR,2))\n",
        "print(\"Average Micro Recall:\",round(aMiR,2))\n",
        "print(\"Average Macro Recall:\",round(aMaR,2))\n",
        "\n",
        "print(\"Average Micro FScore:\",round(aMiF,2))\n",
        "print(\"Average Macro FScore:\",round(aMaF,2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 1 -> Micro-precision: 0.38 \tRecall: 0.38 \tFScore: 0.38\n",
            "Label 2 -> Micro-precision: 0.41 \tRecall: 0.41 \tFScore: 0.41\n",
            "Label 3 -> Micro-precision: 0.4 \tRecall: 0.4 \tFScore: 0.4\n",
            "Label 4 -> Micro-precision: 0.42 \tRecall: 0.42 \tFScore: 0.42\n",
            "Label 5 -> Micro-precision: 0.4 \tRecall: 0.4 \tFScore: 0.4\n",
            "Label 6 -> Micro-precision: 0.41 \tRecall: 0.41 \tFScore: 0.41\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Label 1 -> Macro-precision: 0.2 \tRecall: 0.55 \tFScore: 0.28\n",
            "Label 2 -> Macro-precision: 0.15 \tRecall: 0.36 \tFScore: 0.21\n",
            "Label 3 -> Macro-precision: 0.19 \tRecall: 0.51 \tFScore: 0.28\n",
            "Label 4 -> Macro-precision: 0.15 \tRecall: 0.39 \tFScore: 0.22\n",
            "Label 5 -> Macro-precision: 0.18 \tRecall: 0.47 \tFScore: 0.26\n",
            "Label 6 -> Macro-precision: 0.18 \tRecall: 0.34 \tFScore: 0.21\n",
            "\n",
            "Average Micro Precision: 0.4\n",
            "Average Macro Precision: 0.18\n",
            "Average Micro Recall: 0.4\n",
            "Average Micro Recall: 0.4\n",
            "Average Macro Recall: 0.44\n",
            "Average Micro FScore: 0.4\n",
            "Average Macro FScore: 0.24\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
