{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7_tfidf_202011029.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsRjwUuTT3wX"
      },
      "source": [
        "# Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ3I4WcD3ASo",
        "outputId": "5bb3bd5c-ff6f-4b38-fa3d-012aabf54faf"
      },
      "source": [
        "!gdown --id 18l6IwSqavnqtLQpVnrRqOugZf9XkhEAN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18l6IwSqavnqtLQpVnrRqOugZf9XkhEAN\n",
            "To: /content/jigsaw-toxic-comment-classification-challenge.zip\n",
            "55.2MB [00:00, 67.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbp9GNFDT5cv"
      },
      "source": [
        "# Extracting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoRJNfA3n6S",
        "outputId": "4d059e4f-042d-4d7d-8307-a80bdc5198eb"
      },
      "source": [
        "!unzip jigsaw-toxic-comment-classification-challenge.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  jigsaw-toxic-comment-classification-challenge.zip\n",
            "  inflating: sample_submission.csv.zip  \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: test_labels.csv.zip     \n",
            "  inflating: train.csv.zip           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zvGmtfd4Zxs",
        "outputId": "b577c10d-df39-4cb7-f816-052a4740da0e"
      },
      "source": [
        "!unzip sample_submission.csv.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMFVSiIU4egQ",
        "outputId": "37c1c3ea-9b79-4e29-8da3-672e76791593"
      },
      "source": [
        "!unzip test.csv.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw28JShG4jmi",
        "outputId": "5f93d043-d118-435e-86d1-1080f9d55f5c"
      },
      "source": [
        "!unzip test_labels.csv.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test_labels.csv.zip\n",
            "  inflating: test_labels.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OxKx80Q4n9Q",
        "outputId": "67e7fa58-9940-4b77-89b2-eb0eec22af82"
      },
      "source": [
        "!unzip train.csv.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x2GcrMnT9qH"
      },
      "source": [
        "# Importing and Downloading necessary packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3wqVHYN4r0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4100d411-12f3-4efe-8b2b-abb6398fdf3c"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "import lxml.html\n",
        "import re\n",
        "import numpy as np    \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import bisect\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy\n",
        "from gensim.models import Word2Vec\n",
        "import csv\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJuxsU6yUAlV"
      },
      "source": [
        "# Data from csv files\n",
        "In this section, data is extracted from train.csv and test.csv file using csv.reader and stores in a list where each row contains all the fields of each csv row. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_qcPG941fL"
      },
      "source": [
        "fields1 = []\n",
        "train_data = []\n",
        " \n",
        "with open(\"train.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields1 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    train_data.append(row)\n",
        "\n",
        "fields2 = []\n",
        "test_data = []\n",
        "  \n",
        "with open(\"test.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields2 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    test_data.append(row)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxGTIfstUC9O"
      },
      "source": [
        "# Data Preprocessing\n",
        "In this section, data preprocessing is done on training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CZc9EH3WPES",
        "outputId": "282bcb0d-731c-45fb-8692-18e2837cf377"
      },
      "source": [
        "files = []\n",
        "corpus = []\n",
        "\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "postag = nltk.corpus.wordnet\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for row in train_data :\n",
        "  raw_text = row[1]\n",
        "  for sym in raw_text : \n",
        "    # Removing punctuation\n",
        "    if sym in punc : \n",
        "      raw_text = raw_text.replace(sym, \"\")\n",
        "  # Removing non-alphabetic text and Converting text to lower-case\n",
        "  words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "  # Removing stopwords\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  # Performing lemmatization\n",
        "  lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "  temp_files = []\n",
        "  temp_files.append(row[0])\n",
        "  temp_files.append(lemmatized)\n",
        "  # Storing file data to list\n",
        "  files.append(temp_files)\n",
        "  doc_text = \"\"\n",
        "  for temp_str in lemmatized:\n",
        "    doc_text = doc_text + temp_str + \" \"\n",
        "  corpus.append(doc_text)\n",
        "  \n",
        "print(\"Total Documents: \", len(files))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  159571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfg6ClgwUJAk"
      },
      "source": [
        "# Creating document vectors of corpus using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncQTLVyYYVnw"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vamvx9ReUSbj"
      },
      "source": [
        "# Storing training labels\n",
        "In this section, all the six labels from training data are loaded in separate lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxHsxaszaZuS"
      },
      "source": [
        "train_labels1 = []\n",
        "train_labels2 = []\n",
        "train_labels3 = []\n",
        "train_labels4 = []\n",
        "train_labels5 = []\n",
        "train_labels6 = []\n",
        "\n",
        "for row in train_data : \n",
        "  train_labels1.append(row[2])\n",
        "  train_labels2.append(row[3])\n",
        "  train_labels3.append(row[4])\n",
        "  train_labels4.append(row[5])\n",
        "  train_labels5.append(row[6])\n",
        "  train_labels6.append(row[7])\n",
        "\n",
        "\n",
        "y1 = np.array(train_labels1)\n",
        "y2 = np.array(train_labels2)\n",
        "y3 = np.array(train_labels3)\n",
        "y4 = np.array(train_labels4)\n",
        "y5 = np.array(train_labels5)\n",
        "y6 = np.array(train_labels6)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG7XBH8OUTV8"
      },
      "source": [
        "# Implementing SVM for classification using LinearSVC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrD_O3zqpyoV",
        "outputId": "76d1bf7e-97fe-4821-c97c-38b02ef9f1df"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf1 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf1.fit(X, y1)\n",
        "\n",
        "clf2 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf2.fit(X, y2)\n",
        "\n",
        "clf3 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf3.fit(X, y3)\n",
        "\n",
        "clf4 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf4.fit(X, y4)\n",
        "\n",
        "clf5 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf5.fit(X, y5)\n",
        "\n",
        "clf6 = make_pipeline(LinearSVC(random_state=0, tol=1e-5, verbose=True, max_iter=100))\n",
        "clf6.fit(X, y6)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('linearsvc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=100,\n",
              "                           multi_class='ovr', penalty='l2', random_state=0,\n",
              "                           tol=1e-05, verbose=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0FccVMUf9m"
      },
      "source": [
        "# Accuracy of trained data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y9IG5_cpyoW",
        "outputId": "2495c323-71fd-4d71-94f3-3c80551dfa57"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Optional training accuracy\n",
        "\n",
        "predicted = clf1.predict(X)\n",
        "print(\"Training accuracy using sklearn LinearSVC:\", round(accuracy_score(y1, predicted)*100,2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy using sklearn LinearSVC: 98.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_QDuAzUmr7"
      },
      "source": [
        "# Text preprocessing on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5FoDFF2pyoW",
        "outputId": "46e79a29-b930-4ddd-e2aa-91291d45bca1"
      },
      "source": [
        "files2 = []\n",
        "corpus = []\n",
        "\n",
        "for row in test_data :\n",
        "  raw_text = row[1]\n",
        "  for sym in raw_text : \n",
        "    # Removing punctuation\n",
        "    if sym in punc : \n",
        "      raw_text = raw_text.replace(sym, \"\")\n",
        "  # Removing non-alphabetic text and Converting text to lower-case\n",
        "  words = [word.lower() for word in raw_text.split() if word.isalpha()]\n",
        "  # Removing stopwords\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  # Performing lemmatization\n",
        "  lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
        "  temp_files = []\n",
        "  temp_files.append(row[0])\n",
        "  temp_files.append(lemmatized)\n",
        "  # Storing file data to list\n",
        "  files2.append(temp_files)\n",
        "\n",
        "  doc_text = \"\"\n",
        "  for temp_str in lemmatized:\n",
        "    doc_text = doc_text + temp_str + \" \"\n",
        "  corpus.append(doc_text)\n",
        "\n",
        "  \n",
        "print(\"Total Documents: \", len(files2))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Documents:  153164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOIhejCQUsLy"
      },
      "source": [
        "# Creating vectors of test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j51UWko4pyoX"
      },
      "source": [
        "X = vectorizer.transform(corpus)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHQyCSU3Uv-q"
      },
      "source": [
        "# Placing the predicted labels into output file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv8NFeSypyoX"
      },
      "source": [
        "predicted1 = clf1.predict(X)\n",
        "predicted2 = clf2.predict(X)\n",
        "predicted3 = clf3.predict(X)\n",
        "predicted4 = clf4.predict(X)\n",
        "predicted5 = clf5.predict(X)\n",
        "predicted6 = clf6.predict(X)\n",
        "\n",
        "\n",
        "final_csv = []\n",
        "header = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "final_csv.append(header)\n",
        "\n",
        "for i in range(len(predicted1)):\n",
        "    final_csv.append([files2[i][0], predicted1[i], predicted2[i], predicted3[i], predicted4[i], predicted5[i], predicted6[i]])\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('output_tfidf.csv','w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(final_csv)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHKcimUrU7CI"
      },
      "source": [
        "# Loading the target labels from test_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNvTCMTnpyoY"
      },
      "source": [
        "test_labels = []\n",
        "\n",
        "with open(\"test_labels.csv\", 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  fields2 = next(csvreader)\n",
        "  for row in csvreader:\n",
        "    test_labels.append(row)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1_F8Pb-U7xa"
      },
      "source": [
        "# Removing the rows with -1 value from target and predicted test labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTbNjPv4pyoY"
      },
      "source": [
        "y_pred1 = []\n",
        "y_pred2 = []\n",
        "y_pred3 = []\n",
        "y_pred4 = []\n",
        "y_pred5 = []\n",
        "y_pred6 = []\n",
        "\n",
        "y_actual1 = []\n",
        "y_actual2 = []\n",
        "y_actual3 = []\n",
        "y_actual4 = []\n",
        "y_actual5 = []\n",
        "y_actual6 = []\n",
        "\n",
        "for i in range(len(predicted1)):\n",
        "    if(test_labels[i][1] != -1):\n",
        "        y_pred1.append(predicted1[i])\n",
        "        y_pred2.append(predicted2[i])\n",
        "        y_pred3.append(predicted3[i])\n",
        "        y_pred4.append(predicted4[i])\n",
        "        y_pred5.append(predicted5[i])\n",
        "        y_pred6.append(predicted6[i])\n",
        "\n",
        "        y_actual1.append(test_labels[i][1])\n",
        "        y_actual2.append(test_labels[i][2])\n",
        "        y_actual3.append(test_labels[i][3])\n",
        "        y_actual4.append(test_labels[i][4])\n",
        "        y_actual5.append(test_labels[i][5])\n",
        "        y_actual6.append(test_labels[i][6])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbmBrc6QU_JV"
      },
      "source": [
        "# Finding out the Micro and Macro - Precision, Recall and F1Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049wO0mRpyoa",
        "outputId": "f7f1633b-d0cb-4dfc-93e3-401f3eb54c7d"
      },
      "source": [
        "m1 = precision_recall_fscore_support(y_actual1, y_pred1, average='micro')\n",
        "m2 = precision_recall_fscore_support(y_actual2, y_pred2, average='micro')\n",
        "m3 = precision_recall_fscore_support(y_actual3, y_pred3, average='micro')\n",
        "m4 = precision_recall_fscore_support(y_actual4, y_pred4, average='micro')\n",
        "m5 = precision_recall_fscore_support(y_actual5, y_pred5, average='micro')\n",
        "m6 = precision_recall_fscore_support(y_actual6, y_pred6, average='micro')\n",
        "\n",
        "print(\"Label 1 -> Micro-precision:\", round(m1[0], 2), \"\\tRecall:\", round(m1[1], 2), \"\\tFScore:\", round(m1[2], 2))\n",
        "print(\"Label 2 -> Micro-precision:\", round(m2[0], 2), \"\\tRecall:\", round(m2[1], 2), \"\\tFScore:\", round(m2[2], 2))\n",
        "print(\"Label 3 -> Micro-precision:\", round(m3[0], 2), \"\\tRecall:\", round(m3[1], 2), \"\\tFScore:\", round(m3[2], 2))\n",
        "print(\"Label 4 -> Micro-precision:\", round(m4[0], 2), \"\\tRecall:\", round(m4[1], 2), \"\\tFScore:\", round(m4[2], 2))\n",
        "print(\"Label 5 -> Micro-precision:\", round(m5[0], 2), \"\\tRecall:\", round(m5[1], 2), \"\\tFScore:\", round(m5[2], 2))\n",
        "print(\"Label 6 -> Micro-precision:\", round(m6[0], 2), \"\\tRecall:\", round(m6[1], 2), \"\\tFScore:\", round(m6[2], 2))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "ma1 = precision_recall_fscore_support(y_actual1, y_pred1, average='macro')\n",
        "ma2 = precision_recall_fscore_support(y_actual2, y_pred2, average='macro')\n",
        "ma3 = precision_recall_fscore_support(y_actual3, y_pred3, average='macro')\n",
        "ma4 = precision_recall_fscore_support(y_actual4, y_pred4, average='macro')\n",
        "ma5 = precision_recall_fscore_support(y_actual5, y_pred5, average='macro')\n",
        "ma6 = precision_recall_fscore_support(y_actual6, y_pred6, average='macro')\n",
        "\n",
        "print(\"Label 1 -> Macro-precision:\", round(ma1[0], 2), \"\\tRecall:\", round(ma1[1], 2), \"\\tFScore:\", round(ma1[2], 2))\n",
        "print(\"Label 2 -> Macro-precision:\", round(ma2[0], 2), \"\\tRecall:\", round(ma2[1], 2), \"\\tFScore:\", round(ma2[2], 2))\n",
        "print(\"Label 3 -> Macro-precision:\", round(ma3[0], 2), \"\\tRecall:\", round(ma3[1], 2), \"\\tFScore:\", round(ma3[2], 2))\n",
        "print(\"Label 4 -> Macro-precision:\", round(ma4[0], 2), \"\\tRecall:\", round(ma4[1], 2), \"\\tFScore:\", round(ma4[2], 2))\n",
        "print(\"Label 5 -> Macro-precision:\", round(ma5[0], 2), \"\\tRecall:\", round(ma5[1], 2), \"\\tFScore:\", round(ma5[2], 2))\n",
        "print(\"Label 6 -> Macro-precision:\", round(ma6[0], 2), \"\\tRecall:\", round(ma6[1], 2), \"\\tFScore:\", round(ma6[2], 2))\n",
        "\n",
        "aMiP = (m1[0] + m2[0] + m3[0] + m4[0] + m5[0] + m6[0])/6\n",
        "aMaP = (ma1[0] + ma2[0] + ma3[0] + ma4[0] + ma5[0] + ma6[0])/6\n",
        "aMiR = (m1[1] + m2[1] + m3[1] + m4[1] + m5[1] + m6[1])/6\n",
        "aMaR = (ma1[1] + ma2[1] + ma3[1] + ma4[1] + ma5[1] + ma6[1])/6\n",
        "aMiF = (m1[2] + m2[2] + m3[2] + m4[2] + m5[2] + m6[2])/6\n",
        "aMaF = (ma1[2] + ma2[2] + ma3[2] + ma4[2] + ma5[2] + ma6[2])/6\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Average Micro Precision:\",round(aMiP,2))\n",
        "print(\"Average Macro Precision:\",round(aMaP,2))\n",
        "\n",
        "print(\"Average Micro Recall:\",round(aMiR,2))\n",
        "print(\"Average Micro Recall:\",round(aMiR,2))\n",
        "print(\"Average Macro Recall:\",round(aMaR,2))\n",
        "\n",
        "print(\"Average Micro FScore:\",round(aMiF,2))\n",
        "print(\"Average Macro FScore:\",round(aMaF,2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 1 -> Micro-precision: 0.39 \tRecall: 0.39 \tFScore: 0.39\n",
            "Label 2 -> Micro-precision: 0.41 \tRecall: 0.41 \tFScore: 0.41\n",
            "Label 3 -> Micro-precision: 0.4 \tRecall: 0.4 \tFScore: 0.4\n",
            "Label 4 -> Micro-precision: 0.42 \tRecall: 0.42 \tFScore: 0.42\n",
            "Label 5 -> Micro-precision: 0.4 \tRecall: 0.4 \tFScore: 0.4\n",
            "Label 6 -> Micro-precision: 0.41 \tRecall: 0.41 \tFScore: 0.41\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Label 1 -> Macro-precision: 0.2 \tRecall: 0.58 \tFScore: 0.29\n",
            "Label 2 -> Macro-precision: 0.17 \tRecall: 0.44 \tFScore: 0.24\n",
            "Label 3 -> Macro-precision: 0.2 \tRecall: 0.56 \tFScore: 0.28\n",
            "Label 4 -> Macro-precision: 0.2 \tRecall: 0.43 \tFScore: 0.28\n",
            "Label 5 -> Macro-precision: 0.19 \tRecall: 0.52 \tFScore: 0.28\n",
            "Label 6 -> Macro-precision: 0.2 \tRecall: 0.44 \tFScore: 0.27\n",
            "\n",
            "Average Micro Precision: 0.41\n",
            "Average Macro Precision: 0.19\n",
            "Average Micro Recall: 0.41\n",
            "Average Micro Recall: 0.41\n",
            "Average Macro Recall: 0.49\n",
            "Average Micro FScore: 0.41\n",
            "Average Macro FScore: 0.27\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}